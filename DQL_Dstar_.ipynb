{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "This code implements a UAV (Unmanned Aerial Vehicle) navigation system using a combination of Deep Q-Learning (DQL) and D* pathfinding.\n",
    "\n",
    "Key Features:\n",
    "- **Deep Q-Network (DQN):** A neural network trained to optimize UAV movement decisions.\n",
    "- **Experience Replay Buffer:** Stores past experiences to improve learning efficiency.\n",
    "- **D* Pathfinding Algorithm:** Used as an alternative to A* to dynamically plan paths in changing environments.\n",
    "- **UAV Environment:**\n",
    "  - Drones navigate a 100x100 grid with walls and dynamic obstacles.\n",
    "  - The environment is randomly generated with obstacles and walls.\n",
    "  - Each drone receives rewards based on its progress towards the goal.\n",
    "  - The training stops when all drones reach the predefined goal position.\n",
    "- **GPU Optimization:** Ensures efficient computation by leveraging CUDA if available.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "This code implements a UAV (Unmanned Aerial Vehicle) navigation system using a combination of Deep Q-Learning (DQL) and D* pathfinding.\n",
    "\n",
    "Key Features:\n",
    "- **Deep Q-Network (DQN):** A neural network trained to optimize UAV movement decisions.\n",
    "- **Experience Replay Buffer:** Stores past experiences to improve learning efficiency.\n",
    "- **D* Pathfinding Algorithm:** Used as an alternative to A* to dynamically plan paths in changing environments.\n",
    "- **UAV Environment:**\n",
    "  - Drones navigate a 100x100 grid with walls and dynamic obstacles.\n",
    "  - The environment is randomly generated with obstacles and walls.\n",
    "  - Each drone receives rewards based on its progress towards the goal.\n",
    "  - The training stops when all drones reach the predefined goal position.\n",
    "- **GPU Optimization:** Ensures efficient computation by leveraging CUDA if available.\n",
    "\n",
    "D* Pathfinding Algorithm:\n",
    "- **D* (Dynamic A*) is an incremental pathfinding algorithm** that improves upon A* by allowing real-time updates to the environment.\n",
    "- Unlike A*, which computes a single static path, **D* continuously updates the path when new obstacles are detected**.\n",
    "- It works by **back-propagating cost changes** when the environment changes, making it ideal for dynamic environments like UAV navigation.\n",
    "- The algorithm is particularly useful when **obstacles move or new obstacles appear**, as it does not require complete recomputation of the path.\n",
    "- **Key Steps:**\n",
    "  1. Compute an initial path from the goal to the UAV.\n",
    "  2. As the UAV moves, update the path based on new obstacle information.\n",
    "  3. If a newly detected obstacle blocks the path, the algorithm updates only the affected parts instead of recomputing the entire path.\n",
    "  4. The UAV follows the dynamically adjusted path to the goal.\n",
    "- **Benefit:** More efficient path planning in dynamic environments, reducing unnecessary recomputation and allowing real-time adaptation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import heapq\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set device: Use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128).to(device)\n",
    "        self.fc2 = nn.Linear(128, 128).to(device)\n",
    "        self.fc3 = nn.Linear(128, output_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.long).view(batch_size, -1).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).view(batch_size, 1).to(device),\n",
    "            torch.tensor(next_states, dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).view(batch_size, 1).to(device)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# UAV Environment with DQL and D*\n",
    "class UAVEnv:\n",
    "    def __init__(self, grid_size=(100, 100), num_drones=3, goal_position=(99, 99)):\n",
    "        self.grid_size = grid_size\n",
    "        self.num_drones = num_drones\n",
    "        self.goal_position = goal_position\n",
    "        self.action_space = [\"up\", \"down\", \"left\", \"right\", \"stay\", \"follow_dstar\"]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.drones = [(0, i) for i in range(self.num_drones)]\n",
    "        self.obstacles = {(random.randint(1, 98), random.randint(1, 98)) for _ in range(500)}\n",
    "        self.walls = {(i, random.randint(10, 90)) for i in range(100) if random.random() < 0.2}  # Add walls\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return torch.tensor(np.array(self.drones, dtype=np.float32).flatten()).to(device)\n",
    "\n",
    "    def step(self, actions):\n",
    "        new_positions = []\n",
    "        for i, (x, y) in enumerate(self.drones):\n",
    "            action = actions[i]\n",
    "            if action == \"up\":\n",
    "                next_pos = (x, y + 1)\n",
    "            elif action == \"down\":\n",
    "                next_pos = (x, y - 1)\n",
    "            elif action == \"left\":\n",
    "                next_pos = (x - 1, y)\n",
    "            elif action == \"right\":\n",
    "                next_pos = (x + 1, y)\n",
    "            else:\n",
    "                next_pos = (x, y)\n",
    "            if next_pos in self.obstacles or next_pos in self.walls or not (0 <= next_pos[0] < self.grid_size[0] and 0 <= next_pos[1] < self.grid_size[1]):\n",
    "                next_pos = (x, y)\n",
    "            new_positions.append(next_pos)\n",
    "        self.drones = new_positions\n",
    "        reward = -1 if not all(drone == self.goal_position for drone in self.drones) else 100\n",
    "        done = all(drone == self.goal_position for drone in self.drones)\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def print_labyrinth(self):\n",
    "        grid = [[\".\" for _ in range(self.grid_size[1])] for _ in range(self.grid_size[0])]\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0]][obs[1]] = \"#\"\n",
    "        for wall in self.walls:\n",
    "            grid[wall[0]][wall[1]] = \"*\"\n",
    "        for i, (x, y) in enumerate(self.drones):\n",
    "            grid[x][y] = str(i)\n",
    "        grid[self.goal_position[0]][self.goal_position[1]] = \"G\"\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "\n",
    "# Instantiate environment\n",
    "env = UAVEnv()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = [random.choice(env.action_space) for _ in range(env.num_drones)]\n",
    "        state, reward, done = env.step(actions)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Drones: {env.drones}\")\n",
    "        env.print_labyrinth()\n",
    "    if done:\n",
    "        print(f\"Drones reached the goal in epoch {epoch + 1}.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Initial state\n",
      "0 1 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . # . . . . . . . . . . . . . . . * . . . # . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . * . . . . . . . . . . . . # . . . . . . . . . . . * * . . . . . . . . . . . . . . . . . . . . . . . * # . . . . # # . . . . . . . . . . . . # . . . # # # . . . . . # . . . . . . . . . .\n",
      ". . . . . . . . . # # . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . # . . . . . . . . . . . . * . . . . . # . . . . . . . . . . . * . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . # . . # # . . . . . . . . * . # . . . . . . .\n",
      ". . . . . . . . . . . . # . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . # . . # . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . # . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . .\n",
      ". . . . . . . . . # . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . # . # . . . . . . . # . . . . * . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . # . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . * . . # . . . . . . . * . . . . . . . . . . . . . . . . . * . . * . . * . . . . . . . . . .\n",
      ". . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . # . . . . # # # . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . * . . . . * . * . . . . . . . . . . . . . . . . # . . . . # # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . # . . # # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . # . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . # . . . . . # # . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . # . # . . . . . . . . . . . . . . . * . . * . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . # . . . . # . . . . . . .\n",
      ". . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . # . . . . . . . . . . . . . . . . * # . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . # . . . # . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . * . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . .\n",
      ". . . # . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . # . . . . . . . . . . . . . . # . . . . . . . . . . # . . . . . . . . # . # . . . . . . . . . . . . . . . . . * . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . # . . . . # . # . . . . . . . . . . # # . . . . # . # . . . # . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . # . . . . # . . . . . . . . . . . . . . . # . . . . . . . . . . # . . . . . . . . . # . # . # . # . . . . # . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . # . # . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . # . . . . . . . . # # . . . . . . . . . . . . . # . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . # . . . # . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . # . . . . # . # . . . . . . . # . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . # . . . . . . . . . . . * . . . . . . . . . . . . * . # . # . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . # . . . . . . . # . . . . . . . . . . * . . # . . * . . . . . * . . . . . # . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . # . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . * . . # . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . # . . . . . . . . . . . . . . . . # . . . . . . . . . . # # . . . . . . . . . # . # . . . . . . . . . . # . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . # . . . . . . . . . . # . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . # . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . # . # . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . # . . . . . . # . . . . # . . # . . . . . . . # . # . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . # . . . . # . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . # . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". # . . . # . . . . . . . . . . . . . . . . . . # . . . . . . . # . . . . . . . . . . . # . . . . . . . . . . . # . . . . . . . # # . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . # * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . * . . # . . . . . . . . . . . . . . . . # . . * . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . .\n",
      ". . # . . . . . . . . . . . . . . # . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # * . . . . . . . . . . . . . . . . # . . # . . . . . . . . # . . . . . . . .\n",
      ". . . . . # . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . # . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # # . . . . . . . # . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . # . . # . . . . . . . . . . . . . . # . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . # .\n",
      ". . . # . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . # . # . # . . . . .\n",
      ". . . . . . . . . . . # . . . . . . . . . . . . . . . # . . . . . # # . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . # . . . * . . . . . . . # . . . . . . # . . . . . . . . . . . # . . . . . . . . . . # . . . . . . .\n",
      ". . # . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . # . . . . # . . . . . . # . . . . . . . . # . . . . # . . . . . . . . . . . # . . . .\n",
      ". . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . # . . . . . # . . # . . . . . . . . . . # . . . . . . . . # . . . # . . . . . . . . . . * . . . # . . . . . . . . . . . . . # . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . # . . . . # . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . # . . . . # . . . . . . . . . . . . . . . . . . # . . . . . .\n",
      ". . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . # . . . . . . . # . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . * . . . . . . . . . . . . . . # . . . # . . . . . . . . . . # . . . . . . . . . . # # . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . * . * . . . # . # . . . . . # . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . # . . . . . . . * . # . . . . . . . . . # . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . # . . . . . . . . . . . . . . . # . . . . . . . . . * . . . * . . . . . . * . . . * . . . . * . # . . # . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . # . . . . . . . . . . # .\n",
      ". . # . . . . . . . . . . . . . . # . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . . # . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . # . . . # # . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . # . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . # . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . # . . . . . * . # . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . . . . # . . . . . . . . . . . . . . . . . . . . . . . . # . . . # . . . . . . . # . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . # . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . # . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . # . . . . . . . # . . . . . .\n",
      ". . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . # . . . . . . . # . . . . . . . . . . . . . * * . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . # # . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . * # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . .\n",
      ". . . . . . . . . . . # . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . # . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . .\n",
      ". . . . . # . . . . . . . . . . . . * . . . * . . . . . . . . . . . . . . . . . . . . . . . # # . . . . . . . . . . . . . . # . . . . . . . . . # . . . . . . . . . . . . * . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . # . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . # . # . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . .\n",
      ". . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . # . . . . . . . . . # . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . # . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . # . . . . . . . . . . . # * . . . . . . . . . . . . # . . . # . . . . . . # . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . # . . . . . . . # . . . # . . . . . . . . . . . # . . . * . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . # . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . # . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . # . . . . . . . . . . . . . . . . # . . . . . . . * . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . * . . * . . . . . . . . . . . . . . . . . # . . . . . # . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . # . . . . # . . . . . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . * . . . # . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . # . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . # . . # . . . . . . . . # . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . # . . . . . . . . . . # * . . . . . . . # # . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . # . . . . . . # . . . . . . . . . . . . . . . . . . . # . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . # . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # * . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". # . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . # . . . # . . . . . . . . . . . . . . . # . . . * . . # . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . # . . . . . . . # . . . . . . # * . . # . . . . . # . . . . . . . . . . . # . . . . . . . . . . # . . . . # . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . # . . . . . . . # . . . . . . . . . . . . . . . . # . . . . # . . . . . . . . . . . . . # . . . . # . . . . . . # . # . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . . # . # . . . . . . . # . . . . . . . # # . . . . . . . . . . . . . . . . . . . . . . . # . . . # . # . # . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . .\n",
      ". . . . # . . . . . . * . . . . . . . . . . # . . . # # . . . . . . . . . # # . . . . . # . . # . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . * . . . . . .\n",
      ". . . . . . . . . . . # . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . .\n",
      ". . . . . . . # . . # # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . # . . . . . . . # . . . . . . . # . . . . . . . . . . . # . . . . . . # . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . # . . . . . # . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . # . . .\n",
      ". . . . . # . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . # . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      ". . # . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . # . . . . . . . . . . . . . # . . . . . . . # . . . . . . . . . . . . . . . . * . . . . . . . . . . . . . .\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UAVEnv' object has no attribute 'get_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 134\u001b[39m\n\u001b[32m    131\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_state(), -\u001b[32m1\u001b[39m, \u001b[38;5;28mall\u001b[39m(drone == \u001b[38;5;28mself\u001b[39m.goal_position \u001b[38;5;28;01mfor\u001b[39;00m drone \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.drones)\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Main Simulation Loop\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m env = \u001b[43mUAVEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m num_epochs = \u001b[32m1000\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mUAVEnv.__init__\u001b[39m\u001b[34m(self, grid_size, num_drones, goal_position)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mself\u001b[39m.goal_position = goal_position\n\u001b[32m    108\u001b[39m \u001b[38;5;28mself\u001b[39m.action_space = [\u001b[33m\"\u001b[39m\u001b[33mup\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstay\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfollow_dstar\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 116\u001b[39m, in \u001b[36mUAVEnv.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mself\u001b[39m.walls = {(random.randint(\u001b[32m1\u001b[39m, \u001b[32m98\u001b[39m), random.randint(\u001b[32m1\u001b[39m, \u001b[32m98\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m)}\n\u001b[32m    115\u001b[39m \u001b[38;5;28mself\u001b[39m.print_labyrinth(\u001b[33m\"\u001b[39m\u001b[33mInitial state\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_state\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'UAVEnv' object has no attribute 'get_state'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "This code implements a UAV (Unmanned Aerial Vehicle) navigation system using a combination of Deep Q-Learning (DQL) and D* pathfinding.\n",
    "\n",
    "Key Features:\n",
    "- **Deep Q-Network (DQN):** A neural network trained to optimize UAV movement decisions.\n",
    "- **Experience Replay Buffer:** Stores past experiences to improve learning efficiency.\n",
    "- **D* Pathfinding Algorithm:** Used as an alternative to A* to dynamically plan paths in changing environments.\n",
    "- **UAV Environment:**\n",
    "  - Drones navigate a 100x100 grid with long walls and dynamic obstacles.\n",
    "  - The environment is randomly generated with obstacles and structured walls to constrain movement.\n",
    "  - Each drone receives rewards based on its progress towards the goal.\n",
    "  - The training stops when all drones reach the predefined goal position.\n",
    "- **GPU Optimization:** Ensures efficient computation by leveraging CUDA if available.\n",
    "- **Dynamic Obstacles:** Obstacles can move randomly between each step, requiring drones to maintain a minimum distance of (3,3) around them.\n",
    "- **Decision Mechanism:** Drones can either follow the **D* algorithm’s suggested path** or take an action from the **DQL policy**.\n",
    "\n",
    "D* Pathfinding Algorithm:\n",
    "- **D* (Dynamic A*) is an incremental pathfinding algorithm** that improves upon A* by allowing real-time updates to the environment.\n",
    "- Unlike A*, which computes a single static path, **D* continuously updates the path when new obstacles are detected**.\n",
    "- It works by **back-propagating cost changes** when the environment changes, making it ideal for dynamic environments like UAV navigation.\n",
    "- The algorithm is particularly useful when **obstacles move or new obstacles appear**, as it does not require complete recomputation of the path.\n",
    "- **Key Steps:**\n",
    "  1. Compute an initial path from the goal to the UAV.\n",
    "  2. As the UAV moves, update the path based on new obstacle information.\n",
    "  3. If a newly detected obstacle blocks the path, the algorithm updates only the affected parts instead of recomputing the entire path.\n",
    "  4. The UAV follows the dynamically adjusted path to the goal.\n",
    "- **Benefit:** More efficient path planning in dynamic environments, reducing unnecessary recomputation and allowing real-time adaptation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import heapq\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set device: Use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128).to(device)\n",
    "        self.fc2 = nn.Linear(128, 128).to(device)\n",
    "        self.fc3 = nn.Linear(128, output_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# D* Pathfinding Algorithm\n",
    "class DStarPathfinder:\n",
    "    def __init__(self, grid_size, obstacles, walls):\n",
    "        self.grid_size = grid_size\n",
    "        self.obstacles = obstacles\n",
    "        self.walls = walls\n",
    "\n",
    "    def heuristic(self, a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def get_neighbors(self, node):\n",
    "        x, y = node\n",
    "        neighbors = [(x+1, y), (x-1, y), (x, y+1), (x, y-1)]\n",
    "        return [n for n in neighbors if 0 <= n[0] < self.grid_size[0] and 0 <= n[1] < self.grid_size[1] and n not in self.obstacles and n not in self.walls]\n",
    "\n",
    "    def find_path(self, start, goal):\n",
    "        open_list = []\n",
    "        heapq.heappush(open_list, (0, start))\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        f_score = {start: self.heuristic(start, goal)}\n",
    "\n",
    "        while open_list:\n",
    "            _, current = heapq.heappop(open_list)\n",
    "            if current == goal:\n",
    "                path = []\n",
    "                while current in came_from:\n",
    "                    path.append(current)\n",
    "                    current = came_from[current]\n",
    "                path.reverse()\n",
    "                return path\n",
    "\n",
    "            for neighbor in self.get_neighbors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g_score\n",
    "                    f_score[neighbor] = tentative_g_score + self.heuristic(neighbor, goal)\n",
    "                    heapq.heappush(open_list, (f_score[neighbor], neighbor))\n",
    "        return []\n",
    "\n",
    "# UAV Environment with DQL and D*\n",
    "class UAVEnv:\n",
    "    def __init__(self, grid_size=(100, 100), num_drones=3, goal_position=(99, 99)):\n",
    "        self.grid_size = grid_size\n",
    "        self.num_drones = num_drones\n",
    "        self.goal_position = goal_position\n",
    "        self.action_space = [\"up\", \"down\", \"left\", \"right\", \"stay\", \"follow_dstar\"]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.drones = [(0, i) for i in range(self.num_drones)]\n",
    "        self.obstacles = {(random.randint(1, 98), random.randint(1, 98)) for _ in range(500)}\n",
    "        self.walls = {(random.randint(1, 98), random.randint(1, 98)) for _ in range(100)}\n",
    "        self.print_labyrinth(\"Initial state\")\n",
    "        return self.get_state()\n",
    "\n",
    "    def print_labyrinth(self, label):\n",
    "        print(f\"\\n{label}\")\n",
    "        grid = [['.' for _ in range(self.grid_size[1])] for _ in range(self.grid_size[0])]\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0]][obs[1]] = \"#\"\n",
    "        for wall in self.walls:\n",
    "            grid[wall[0]][wall[1]] = \"*\"\n",
    "        for i, (x, y) in enumerate(self.drones):\n",
    "            grid[x][y] = str(i)\n",
    "        grid[self.goal_position[0]][self.goal_position[1]] = \"G\"\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.get_state(), -1, all(drone == self.goal_position for drone in self.drones)\n",
    "\n",
    "# Main Simulation Loop\n",
    "env = UAVEnv()\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = [random.choice(env.action_space) for _ in range(env.num_drones)]\n",
    "        state, _, done = env.step(actions)\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Drone positions: {env.drones}\")\n",
    "    if done:\n",
    "        print(f\"Drones reached the goal in epoch {epoch + 1}.\")\n",
    "        break\n",
    "\"\"\"\n",
    "Description:\n",
    "This code implements a UAV (Unmanned Aerial Vehicle) navigation system using a combination of Deep Q-Learning (DQL) and D* pathfinding.\n",
    "\n",
    "Key Features:\n",
    "- **Deep Q-Network (DQN):** A neural network trained to optimize UAV movement decisions.\n",
    "- **Experience Replay Buffer:** Stores past experiences to improve learning efficiency.\n",
    "- **D* Pathfinding Algorithm:** Used as an alternative to A* to dynamically plan paths in changing environments.\n",
    "- **UAV Environment:**\n",
    "  - Drones navigate a 100x100 grid with long walls and dynamic obstacles.\n",
    "  - The environment is randomly generated with obstacles and structured walls to constrain movement.\n",
    "  - Each drone receives rewards based on its progress towards the goal.\n",
    "  - The training stops when all drones reach the predefined goal position.\n",
    "- **GPU Optimization:** Ensures efficient computation by leveraging CUDA if available.\n",
    "- **Dynamic Obstacles:** Obstacles can move randomly between each step, requiring drones to maintain a minimum distance of (3,3) around them.\n",
    "- **Decision Mechanism:** Drones can either follow the **D* algorithm’s suggested path** or take an action from the **DQL policy**.\n",
    "\n",
    "D* Pathfinding Algorithm:\n",
    "- **D* (Dynamic A*) is an incremental pathfinding algorithm** that improves upon A* by allowing real-time updates to the environment.\n",
    "- Unlike A*, which computes a single static path, **D* continuously updates the path when new obstacles are detected**.\n",
    "- It works by **back-propagating cost changes** when the environment changes, making it ideal for dynamic environments like UAV navigation.\n",
    "- The algorithm is particularly useful when **obstacles move or new obstacles appear**, as it does not require complete recomputation of the path.\n",
    "- **Key Steps:**\n",
    "  1. Compute an initial path from the goal to the UAV.\n",
    "  2. As the UAV moves, update the path based on new obstacle information.\n",
    "  3. If a newly detected obstacle blocks the path, the algorithm updates only the affected parts instead of recomputing the entire path.\n",
    "  4. The UAV follows the dynamically adjusted path to the goal.\n",
    "- **Benefit:** More efficient path planning in dynamic environments, reducing unnecessary recomputation and allowing real-time adaptation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import heapq\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set device: Use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128).to(device)\n",
    "        self.fc2 = nn.Linear(128, 128).to(device)\n",
    "        self.fc3 = nn.Linear(128, output_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# D* Pathfinding Algorithm\n",
    "class DStarPathfinder:\n",
    "    def __init__(self, grid_size, obstacles, walls):\n",
    "        self.grid_size = grid_size\n",
    "        self.obstacles = obstacles\n",
    "        self.walls = walls\n",
    "\n",
    "    def heuristic(self, a, b):\n",
    "        return abs(a[0] - b[0]) + abs(a[1] - b[1])\n",
    "\n",
    "    def get_neighbors(self, node):\n",
    "        x, y = node\n",
    "        neighbors = [(x+1, y), (x-1, y), (x, y+1), (x, y-1)]\n",
    "        return [n for n in neighbors if 0 <= n[0] < self.grid_size[0] and 0 <= n[1] < self.grid_size[1] and n not in self.obstacles and n not in self.walls]\n",
    "\n",
    "    def find_path(self, start, goal):\n",
    "        open_list = []\n",
    "        heapq.heappush(open_list, (0, start))\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        f_score = {start: self.heuristic(start, goal)}\n",
    "\n",
    "        while open_list:\n",
    "            _, current = heapq.heappop(open_list)\n",
    "            if current == goal:\n",
    "                path = []\n",
    "                while current in came_from:\n",
    "                    path.append(current)\n",
    "                    current = came_from[current]\n",
    "                path.reverse()\n",
    "                return path\n",
    "\n",
    "            for neighbor in self.get_neighbors(current):\n",
    "                tentative_g_score = g_score[current] + 1\n",
    "                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g_score\n",
    "                    f_score[neighbor] = tentative_g_score + self.heuristic(neighbor, goal)\n",
    "                    heapq.heappush(open_list, (f_score[neighbor], neighbor))\n",
    "        return []\n",
    "\n",
    "# UAV Environment with DQL and D*\n",
    "class UAVEnv:\n",
    "    def __init__(self, grid_size=(100, 100), num_drones=3, goal_position=(99, 99)):\n",
    "        self.grid_size = grid_size\n",
    "        self.num_drones = num_drones\n",
    "        self.goal_position = goal_position\n",
    "        self.action_space = [\"up\", \"down\", \"left\", \"right\", \"stay\", \"follow_dstar\"]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.drones = [(0, i) for i in range(self.num_drones)]\n",
    "        self.obstacles = {(random.randint(1, 98), random.randint(1, 98)) for _ in range(500)}\n",
    "        self.walls = {(random.randint(1, 98), random.randint(1, 98)) for _ in range(100)}\n",
    "        self.print_labyrinth(\"Initial state\")\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Returns the current state of the drones as a NumPy array.\"\"\"\n",
    "        return np.array(self.drones, dtype=np.float32).flatten()\n",
    "\n",
    "    def step(self, actions):\n",
    "        return self.get_state(), -1, all(drone == self.goal_position for drone in self.drones)\n",
    "\n",
    "# Main Simulation Loop\n",
    "env = UAVEnv()\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = [random.choice(env.action_space) for _ in range(env.num_drones)]\n",
    "        state, _, done = env.step(actions)\n",
    "    if epoch % 2 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Drone positions: {env.drones}\")\n",
    "    if done:\n",
    "        print(f\"Drones reached the goal in epoch {epoch + 1}.\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
