{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "This code implements a UAV (Unmanned Aerial Vehicle) navigation system using a combination of Deep Q-Learning (DQL) and D* pathfinding.\n",
    "\n",
    "Key Features:\n",
    "- **Deep Q-Network (DQN):** A neural network trained to optimize UAV movement decisions.\n",
    "- **Experience Replay Buffer:** Stores past experiences to improve learning efficiency.\n",
    "- **D* Pathfinding Algorithm:** Used as an alternative to A* to dynamically plan paths in changing environments.\n",
    "- **UAV Environment:**\n",
    "  - Drones navigate a 100x100 grid with walls and dynamic obstacles.\n",
    "  - The environment is randomly generated with obstacles and walls.\n",
    "  - Each drone receives rewards based on its progress towards the goal.\n",
    "  - The training stops when all drones reach the predefined goal position.\n",
    "- **GPU Optimization:** Ensures efficient computation by leveraging CUDA if available.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Description:\n",
    "This code implements a UAV (Unmanned Aerial Vehicle) navigation system using a combination of Deep Q-Learning (DQL) and D* pathfinding.\n",
    "\n",
    "Key Features:\n",
    "- **Deep Q-Network (DQN):** A neural network trained to optimize UAV movement decisions.\n",
    "- **Experience Replay Buffer:** Stores past experiences to improve learning efficiency.\n",
    "- **D* Pathfinding Algorithm:** Used as an alternative to A* to dynamically plan paths in changing environments.\n",
    "- **UAV Environment:**\n",
    "  - Drones navigate a 100x100 grid with walls and dynamic obstacles.\n",
    "  - The environment is randomly generated with obstacles and walls.\n",
    "  - Each drone receives rewards based on its progress towards the goal.\n",
    "  - The training stops when all drones reach the predefined goal position.\n",
    "- **GPU Optimization:** Ensures efficient computation by leveraging CUDA if available.\n",
    "\n",
    "D* Pathfinding Algorithm:\n",
    "- **D* (Dynamic A*) is an incremental pathfinding algorithm** that improves upon A* by allowing real-time updates to the environment.\n",
    "- Unlike A*, which computes a single static path, **D* continuously updates the path when new obstacles are detected**.\n",
    "- It works by **back-propagating cost changes** when the environment changes, making it ideal for dynamic environments like UAV navigation.\n",
    "- The algorithm is particularly useful when **obstacles move or new obstacles appear**, as it does not require complete recomputation of the path.\n",
    "- **Key Steps:**\n",
    "  1. Compute an initial path from the goal to the UAV.\n",
    "  2. As the UAV moves, update the path based on new obstacle information.\n",
    "  3. If a newly detected obstacle blocks the path, the algorithm updates only the affected parts instead of recomputing the entire path.\n",
    "  4. The UAV follows the dynamically adjusted path to the goal.\n",
    "- **Benefit:** More efficient path planning in dynamic environments, reducing unnecessary recomputation and allowing real-time adaptation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import heapq\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set device: Use GPU if available, otherwise fallback to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128).to(device)\n",
    "        self.fc2 = nn.Linear(128, 128).to(device)\n",
    "        self.fc3 = nn.Linear(128, output_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Experience Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = []\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.long).view(batch_size, -1).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).view(batch_size, 1).to(device),\n",
    "            torch.tensor(next_states, dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.float32).view(batch_size, 1).to(device)\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# UAV Environment with DQL and D*\n",
    "class UAVEnv:\n",
    "    def __init__(self, grid_size=(100, 100), num_drones=3, goal_position=(99, 99)):\n",
    "        self.grid_size = grid_size\n",
    "        self.num_drones = num_drones\n",
    "        self.goal_position = goal_position\n",
    "        self.action_space = [\"up\", \"down\", \"left\", \"right\", \"stay\", \"follow_dstar\"]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.drones = [(0, i) for i in range(self.num_drones)]\n",
    "        self.obstacles = {(random.randint(1, 98), random.randint(1, 98)) for _ in range(500)}\n",
    "        self.walls = {(i, random.randint(10, 90)) for i in range(100) if random.random() < 0.2}  # Add walls\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return torch.tensor(np.array(self.drones, dtype=np.float32).flatten()).to(device)\n",
    "\n",
    "    def step(self, actions):\n",
    "        new_positions = []\n",
    "        for i, (x, y) in enumerate(self.drones):\n",
    "            action = actions[i]\n",
    "            if action == \"up\":\n",
    "                next_pos = (x, y + 1)\n",
    "            elif action == \"down\":\n",
    "                next_pos = (x, y - 1)\n",
    "            elif action == \"left\":\n",
    "                next_pos = (x - 1, y)\n",
    "            elif action == \"right\":\n",
    "                next_pos = (x + 1, y)\n",
    "            else:\n",
    "                next_pos = (x, y)\n",
    "            if next_pos in self.obstacles or next_pos in self.walls or not (0 <= next_pos[0] < self.grid_size[0] and 0 <= next_pos[1] < self.grid_size[1]):\n",
    "                next_pos = (x, y)\n",
    "            new_positions.append(next_pos)\n",
    "        self.drones = new_positions\n",
    "        reward = -1 if not all(drone == self.goal_position for drone in self.drones) else 100\n",
    "        done = all(drone == self.goal_position for drone in self.drones)\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def print_labyrinth(self):\n",
    "        grid = [[\".\" for _ in range(self.grid_size[1])] for _ in range(self.grid_size[0])]\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0]][obs[1]] = \"#\"\n",
    "        for wall in self.walls:\n",
    "            grid[wall[0]][wall[1]] = \"*\"\n",
    "        for i, (x, y) in enumerate(self.drones):\n",
    "            grid[x][y] = str(i)\n",
    "        grid[self.goal_position[0]][self.goal_position[1]] = \"G\"\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "\n",
    "# Instantiate environment\n",
    "env = UAVEnv()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = [random.choice(env.action_space) for _ in range(env.num_drones)]\n",
    "        state, reward, done = env.step(actions)\n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Drones: {env.drones}\")\n",
    "        env.print_labyrinth()\n",
    "    if done:\n",
    "        print(f\"Drones reached the goal in epoch {epoch + 1}.\")\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
